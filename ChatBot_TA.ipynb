{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mueble2/NLP-ChatBot-TA/blob/main/ChatBot_TA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ü§ñ **Chatbot Acad√©mico: Historia del Per√∫**"
      ],
      "metadata": {
        "id": "Trua87COVh4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers chromadb langchain langchain-community"
      ],
      "metadata": {
        "id": "bV0I5txRjAnF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üì° Carga de contenido web\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "# üß† Modelo de embedding (sentence transformers)\n",
        "from sentence_transformers import SentenceTransformer\n",
        "# üß† Embeddings y Vector Store\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from chromadb.config import Settings\n",
        "import chromadb\n",
        "# üìÑ Procesamiento de documentos\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "# üîç Motor de recuperaci√≥n de informaci√≥n\n",
        "from langchain.chains import RetrievalQA\n",
        "# ü§ñ LLM v√≠a Ollama\n",
        "from langchain_community.llms import Ollama\n",
        "#Para cargar documentos\n",
        "from langchain_community.document_loaders import PyPDFLoader\n"
      ],
      "metadata": {
        "id": "QqsGHrEiWh6H"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extraer_texto(url):\n",
        "    try:\n",
        "        resp = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)\n",
        "        resp.raise_for_status()  # Lanza excepci√≥n si falla (4xx/5xx)\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"‚ùå Error al acceder a {url}: {e}\")\n",
        "        return \"\"\n",
        "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "    # Elimina etiquetas no deseadas\n",
        "    for tag in soup([\"script\", \"style\", \"sup\", \"img\", \"figure\", \"table\", \"noscript\"]):\n",
        "        tag.decompose()\n",
        "    # Extrae texto limpio\n",
        "    texto = soup.get_text(separator=' ')\n",
        "    texto = re.sub(r'\\s+', ' ', texto)\n",
        "    return texto.strip()\n",
        "\n",
        "urls = [\n",
        "    \"https://es.wikipedia.org/wiki/Batalla_de_Ayacucho\",\n",
        "    \"https://en.wikipedia.org/wiki/Battle_of_Ayacucho\",\n",
        "    \"https://www.encyclopedia.com/humanities/encyclopedias-almanacs-transcripts-and-maps/ayacucho-battle\",\n",
        "    \"https://www.britannica.com/topic/Battle-of-Ayacucho\",\n",
        "    \"https://www.britannica.com/place/Ayacucho-Peru\",\n",
        "    \"https://www.tierrasvivas.com/en/ayacucho-battle\",\n",
        "    \"https://es.wikipedia.org/wiki/Capitulaci%C3%B3n_de_Ayacucho\",\n",
        "    \"https://es.wikipedia.org/wiki/Santuario_hist%C3%B3rico_de_la_Pampa_de_Ayacucho\",\n",
        "    \"https://elpais.com/america/2024-12-09/ayacucho-diciembre-9-1824-el-final-de-un-imperio-y-el-inicio-de-america-latina.html\"\n",
        "]\n",
        "\n",
        "documentos = [extraer_texto(url) for url in urls]\n",
        "print(f\"‚úÖ Se extrajeron {sum(1 for d in documentos if d)} documentos v√°lidos.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ0Qm0I3Xf_e",
        "outputId": "1002a5f7-c2cf-4e82-c044-441cfc817717"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Se extrajeron 9 documentos v√°lidos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Embeddings\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# 2. Chunking por oraciones\n",
        "def dividir_en_chunks(texto, max_len=400):\n",
        "    oraciones = re.split(r'(?<=[.?!])\\s+', texto)\n",
        "    chunks, actual = [], \"\"\n",
        "    for oracion in oraciones:\n",
        "        if len(actual) + len(oracion) <= max_len:\n",
        "            actual += \" \" + oracion\n",
        "        else:\n",
        "            chunks.append(actual.strip())\n",
        "            actual = oracion\n",
        "    if actual:\n",
        "        chunks.append(actual.strip())\n",
        "    return chunks\n",
        "\n",
        "# 3. Procesar todos los documentos\n",
        "chunks = []\n",
        "for doc in documentos:\n",
        "    if doc:  # evita errores por documentos vac√≠os\n",
        "        chunks.extend(dividir_en_chunks(doc))\n",
        "\n",
        "# 4. Generar embeddings\n",
        "vectores = embedding_model.encode(chunks)\n",
        "\n",
        "# 5. Crear cliente Chroma\n",
        "chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
        "\n",
        "# Crear colecci√≥n nueva\n",
        "collection = chroma_client.get_or_create_collection(\"batalla_ayacucho_rag\")\n",
        "\n",
        "# 6. Cargar datos\n",
        "for i, chunk in enumerate(chunks):\n",
        "    collection.add(documents=[chunk], ids=[f\"chunk_{i}\"], embeddings=[vectores[i].tolist()])\n",
        "\n",
        "print(f\"‚úÖ {len(chunks)} chunks embebidos y cargados en Chroma.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57WAUnwDXotH",
        "outputId": "9ddc755b-50d9-4e10-f97a-b45063c05dd3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ 488 chunks embebidos y cargados en Chroma.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Descarga de OLlama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh  # Solo una vez"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6gw5CHQbXyy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lanzar servidor de OLlama en local\n",
        "!ollama serve > /dev/null 2>&1 &\n",
        "!sleep 10"
      ],
      "metadata": {
        "id": "dkrNWLHIX2zI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Descarga del modelo y librer√≠as\n",
        "!ollama pull phi3.5:latest\n",
        "!pip install -U langchain-ollama"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rjWXGNP3X4aH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga del modelo\n",
        "from langchain_ollama import OllamaLLM\n",
        "llm = OllamaLLM(\n",
        "    model=\"phi3.5:latest\",\n",
        "    temperature=0.5,\n",
        "    num_gpu=40,\n",
        "    system=\"\"\n",
        ")"
      ],
      "metadata": {
        "id": "5KQlRQwAX6N7"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def responder_pregunta(pregunta: str, k=3, max_chars=1500) -> str:\n",
        "    # Embedding de la pregunta\n",
        "    query_emb = embedding_model.encode(pregunta).tolist()\n",
        "\n",
        "    # Recuperar documentos similares\n",
        "    resultados = collection.query(\n",
        "        query_embeddings=[query_emb],\n",
        "        n_results=k\n",
        "    )\n",
        "\n",
        "    # Construcci√≥n de contexto\n",
        "    contextos = resultados[\"documents\"][0]\n",
        "    contexto = \"\\n\\n\".join(contextos)[:max_chars]\n",
        "\n",
        "    # Prompt para Ollama + phi3.5\n",
        "    prompt = f\"\"\"Responde en espa√±ol a la siguiente pregunta sobre historia peruana.\n",
        "\n",
        "  Contexto hist√≥rico:\n",
        "  {contexto}\n",
        "\n",
        "  Pregunta: {pregunta}\n",
        "  Respuesta:\"\"\"\n",
        "\n",
        "    # Llamada al modelo local\n",
        "    return llm.invoke(prompt).strip()\n"
      ],
      "metadata": {
        "id": "uv0sKiE3X8aY"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pregunta = \"¬øPor qu√© se gener√≥ la Batalla de Ayacucho?\"\n",
        "respuesta = responder_pregunta(pregunta)\n",
        "\n",
        "print(\"üß† Pregunta:\", pregunta)\n",
        "print(\"üìò Respuesta:\", respuesta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veJ19BNeYFWl",
        "outputId": "127a06d0-92b4-4362-b8a5-1d934877ceb0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Pregunta: ¬øPor qu√© se gener√≥ la Batalla de Ayacucho?\n",
            "üìò Respuesta: La Batalla de Ayacucho fue resultado de un combate civil entre dos bandos en el contexto del proceso independentista peruano. Seg√∫n los testimonios hist√≥dicticos, tanto las fuerzas realistas como las patriotas contaron con auxiliares forasteros que jugaron un papel significativo durante la confrontaci√≥n armada. La batalla se vio influenciada por acusaciones de traici√≥n y manipulaci√≥n pol√≠tica seg√∫n lo relatado por los oficiales espa√±oles, mientras el Per√∫ luchaba por su independencia del dominio espa√±ol. Por tanto, Ayacucho represent√≥ un punto culminante en la guerra de liberaci√≥n que buscaba desafiar las estructuras coloniales y asentar una nueva naci√≥n soberana.\n",
            "\n",
            "La controversia sobre si el desenlace estaba preestablecido o no a√±ade complejidad a nuestra comprensi√≥n hist√≥rica, pero es evidente que la lucha por la autonom√≠a fue un factor central en la g√©nesis de esta batalla decisiva para Am√©rica Latina. La resistencia y los ideales independentistas del pueblo peruano fueron cruciales para el desenlace final contra las fuerzas coloniales espa√±olas lideradas por Andr√©s Garc√≠a Camba, quien estuvo presente como comandante espa√±ol durante dicho conflicto en 1824.\n",
            "\n",
            "En conclusi√≥n, la Batalla de Ayacucho se origin√≥ debido a los intensos desaf√≠os pol√≠ticos y militares que implicaban el proceso independentista del Per√∫ contra las autoridades colonialistas espa√±olas; un momento donde bandos locales lucharon por su soberan√≠a nacional con apoyo externo, culminando en una confrontaci√≥n decisiva para definir la independencia de Am√©rica Latina.\n"
          ]
        }
      ]
    }
  ]
}